这篇论文主要研究了用于JPEG隐写分析的EfficientNet架构的改进。作者通过一系列的“外科手术式修改”显著提高了其性能，同时考虑了计算复杂度，并在不同数据集上进行了测试，具体内容如下：
1. **研究背景和动机**
    - **机器学习隐写分析的发展**：过去五年中，机器学习隐写分析快速发展，从传统的富媒体模型被卷积神经网络（CNNs）取代。基于计算机视觉任务训练的CNNs可作为隐写分析迁移学习的良好起点，在相关竞赛中得到验证。
    - **EfficientNet的应用与不足**：EfficientNet在ALASKA II数据集中的JPEG隐写分析任务上表现出竞争力，但在其他数据集如BOSSbase + BOWS2中表现不佳，这是由于早期层中的下采样操作影响了检测准确性，且不同数据集图像特征存在差异。
2. **研究方法**
    - **符号与度量标准**：使用加权面积下的接收者操作特征曲线（wAUC）评估探测器性能，还提及了最小平均错误率（$P_{E}$）；以FLOPs衡量计算复杂度，通过‘fvcore‘包计算；以峰值内存消耗衡量GPU内存使用情况。
    - **实验设置**
        - **数据集**：使用ALASKA II数据集作为主要实验数据，包含不同质量因子压缩的封面图像和嵌入隐写信息的图像，并分为训练集、验证集和测试集。还使用BOSSbase + BOWS2和ALASKA II BOSS - style数据集进行对比实验。
        - **模型与训练参数**：研究EfficientNet家族和SE - ResNet架构，采用迁移学习方法对预训练模型进行微调，详细介绍了优化器、学习率调度器、训练增强方法、批次大小等训练参数。
    - **EfficientNet的改进**
        - **外科手术式修改**：包括两种类型，一是消融架构中的下采样元素（如步长、池化），二是在高分辨率特征表示处插入额外层。
        - **具体修改操作及效果**
            - **茎步长消融**：去除EfficientNet茎中的步长可提高性能，但会增加FLOPs和内存需求。对SE - ResNet也进行了类似操作，同样增加了计算成本。
            - **非池化层植入**：在架构的关键部分插入层，如“预茎插入”和“后茎插入”，可以在较低的FLOPs和内存成本下提高性能。通过改变设计超参数进一步研究后茎插入，发现增加植入的倒置残差块的扩展参数或使用ResNet块可提高性能，且性能随着插入层数增加而提高，在插入4个ResNet块时达到饱和。
        - **ImageNet预训练的影响**：研究发现对修改后的CNN在ImageNet上进行预训练没有实质性好处，因此可以在迁移学习阶段进行修改。
3. **实验结果**
    - **不同数据集上的性能比较**：在ALASKA II数据集中，改进后的EfficientNet模型达到了最先进的性能，FLOPs低于当前最佳模型的1/2。在BOSSbase + BOWS2和ALASKA II BOSS - style数据集上，原始的ImageNet预训练模型表现不佳，原因是缺乏非池化层，而提出的外科手术式修改可以克服这一问题，使EfficientNet性能优于SRNet。
    - **不同修改方式的比较**：对比了不同的修改方式在性能、FLOPs和内存需求方面的表现，结果表明后茎修改在保持计算成本和内存需求合理的情况下提高了性能，而茎步长消融需要大量GPU内存进行训练。
4. **研究结论**：提出的几种对EfficientNet架构的修改方法能显著提高JPEG隐写分析性能，在不同数据集上克服了原始模型的不足，确认了现成的计算机视觉架构在JPEG隐写分析中可达到优异性能，且除了已知对隐写分析有益的非池化层外，无需添加特殊元素。同时指出研究得到了NSF基金的支持, , , , , , , , 。
